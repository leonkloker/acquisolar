{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install and Import\n",
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "import json\n",
    "import tiktoken\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set Client and Diretory to find documents in (local)\n",
    "client = OpenAI(api_key=\"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\")\n",
    "\n",
    "def set_directory_for_input_document():\n",
    "    # Change the working directory my local one\n",
    "    target_directory = r'/Users/jd/Documents/Coding/AcquiSolar/Metadata_extraction/input/J_test'\n",
    "    os.chdir(target_directory)\n",
    "    print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Text from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## New Version: adds a function to read pages parallel to speed up the process and make it less expensive\n",
    "# ## and should it make easier to read different kinds of documents\n",
    "\n",
    "# def extract_text_from_page(doc, page_num):\n",
    "#     page = doc.load_page(page_num) ## doc = pdf object\n",
    "#     text = page.get_text()\n",
    "#     # Apply any necessary text processing here\n",
    "#     return text\n",
    "\n",
    "# def extract_text_from_pdf_parallel(pdf_name):\n",
    "#     doc = fitz.open(pdf_name)\n",
    "#     full_text = \"\"\n",
    "#     with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "#         # Map each page to the executor\n",
    "#         results = executor.map(lambda p: extract_text_from_page(doc, p), range(len(doc)))\n",
    "#         for text in results:\n",
    "#             full_text += text + \"\\n\"  # Concatenate the results with newlines\n",
    "#     doc.close()\n",
    "#     return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def process_extracted_text(text):\n",
    "    # Remove headers/footers\n",
    "    text = re.sub(r'(?m)^(?:\\d+|[A-Z]+)\\s*(\\r?\\n)\\1', '', text)\n",
    "    # Handle hyphenation at the end of lines\n",
    "    text = re.sub(r'(\\w+)-\\s*\\n\\s*(\\w+)', r'\\1\\2', text)\n",
    "    # Remove line breaks within a paragraph\n",
    "    text = re.sub(r'(?<!\\n)\\n(?!\\n)', ' ', text)\n",
    "    # Remove page numbers\n",
    "    text = re.sub(r'(?m)^\\s*\\d+\\s*\\n', '', text)\n",
    "    return text\n",
    "\n",
    "def extract_text_from_page(doc, page_num):\n",
    "    page = doc.load_page(page_num)  # doc = pdf object\n",
    "    text = page.get_text()\n",
    "    # Apply the text processing here\n",
    "    processed_text = process_extracted_text(text)\n",
    "    return processed_text\n",
    "\n",
    "def extract_text_from_pdf_parallel(pdf_name):\n",
    "    doc = fitz.open(pdf_name)\n",
    "    full_text = \"\"\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        # Map each page to the executor\n",
    "        results = executor.map(lambda p: extract_text_from_page(doc, p), range(len(doc)))\n",
    "        for text in results:\n",
    "            full_text += text + \"\\n\"  # Concatenate the results with newlines\n",
    "    doc.close()\n",
    "    return full_text\n",
    "\n",
    "# Example of using the function:\n",
    "# pdf_text = extract_text_from_pdf_parallel('path_to_your_pdf.pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## New Version\n",
    "\n",
    "\n",
    "\n",
    "# from transformers import GPT2Tokenizer  # Placeholder for GPT-4 tokenizer\n",
    "\n",
    "# def truncate_query_to_fit_context(query, max_length=2000):\n",
    "#     \"\"\"\n",
    "#     Truncate a query using GPT-4 tokenizer to fit within a specified maximum length.\n",
    "    \n",
    "#     Parameters:\n",
    "#     - query (str): The text query to be truncated.\n",
    "#     - max_length (int): The maximum allowed length in tokens.\n",
    "    \n",
    "#     Returns:\n",
    "#     - str: Truncated query.\n",
    "#     \"\"\"\n",
    "#     # Initialize GPT-4 tokenizer (using GPT-2 as a placeholder)\n",
    "#     # For actual implementation, replace 'gpt2' with the appropriate GPT-4 identifier\n",
    "#     tokenizer = GPT2Tokenizer.from_pretrained('gpt2')  # This should be replaced with GPT-4's tokenizer\n",
    "    \n",
    "#     # Tokenize the query\n",
    "#     tokens = tokenizer.encode(query, return_tensors=\"pt\")\n",
    "    \n",
    "#     # Ensure the token array does not exceed max_length\n",
    "#     if tokens.size(1) > max_length:\n",
    "#         # Truncate the tokens to the maximum length\n",
    "#         truncated_tokens = tokens[:, :max_length]\n",
    "#         # Decode tokens back to text\n",
    "#         truncated_query = tokenizer.decode(truncated_tokens[0], clean_up_tokenization_spaces=True)\n",
    "#     else:\n",
    "#         truncated_query = query\n",
    "\n",
    "#     return truncated_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Version 3\n",
    "from transformers import GPT2Tokenizer  \n",
    "\n",
    "\n",
    "def truncate_query_to_fit_context(query, max_length=2000):\n",
    "    \"\"\"\n",
    "    Truncate a query using GPT-2 tokenizer to fit within a specified maximum length.\n",
    "    \n",
    "    Parameters:\n",
    "    - query (str): The text query to be truncated.\n",
    "    - max_length (int): The maximum allowed length in tokens.\n",
    "    \n",
    "    Returns:\n",
    "    - str: Truncated query.\n",
    "    \"\"\"\n",
    "    # Initialize GPT-4 tokenizer (using GPT-2 as a placeholder)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2') \n",
    "    \n",
    "    # Tokenize the query and truncate if needed\n",
    "    tokens = tokenizer.encode(query, return_tensors=\"pt\")\n",
    "    num_tokens = tokens.size(1)\n",
    "    \n",
    "    if num_tokens > max_length:\n",
    "        # Truncate the tokens to the maximum length\n",
    "        truncated_tokens = tokens[:, :max_length].tolist()[0]\n",
    "        # Try to find the last complete sentence to avoid cutting in the middle of a sentence\n",
    "        end_of_sentence_indices = [idx for idx, token_id in enumerate(truncated_tokens) if tokenizer.decode([token_id]) in '.!?']\n",
    "        if end_of_sentence_indices:\n",
    "            last_sentence_index = end_of_sentence_indices[-1] + 1\n",
    "            truncated_tokens = truncated_tokens[:last_sentence_index]\n",
    "        # Decode tokens back to text\n",
    "        truncated_query = tokenizer.decode(truncated_tokens, clean_up_tokenization_spaces=True)\n",
    "    else:\n",
    "        truncated_query = query\n",
    "    \n",
    "    return truncated_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_query(extracted_text):\n",
    "    return f\"\"\"\n",
    "Extract the following fields from the document text provided and format the response as JSON:\n",
    "- \"Document date\" in the format '3 letter month name-DD, YYYY'.\n",
    "- \"Document summary\" limited to a maximum of 3 sentences, tailored for a solar M&A analyst. It should state what kind of document it is, but also what its implicatoins are or what state it is in. It should assume the analyst knows about the M&A process.\n",
    "- \"Document type\", which should be either 'PPA' or 'Interconnection document' or 'email' or 'site control'.\n",
    "- \"Suggested title\" in the format 'MM-DD-YYYY max 5 word document title (state)' the state field is optional. It can read \"main\" if it is said to be the main document of its type, it can read (redacted) if it is redacted.\n",
    "- \"Suggested title v2\" in same format as \"suggested title\" but with different wording\n",
    "- \"Suggested title v3\" in same format as \"suggested title\" but with different wording\n",
    "- \"Suggested folder\" from the selection: \"PPA\", \"interconnection\", \"uncategorized\", \"site control\"\n",
    "\n",
    "\n",
    "The provided document text is:\n",
    "{extracted_text}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_extracted_text_to_file(extracted_text, filename=\"Extracted_text.txt\"):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/jd/Documents/Coding/AcquiSolar/Metadata_extraction/input/J_test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21391 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Document date\": \"Aug-03, 2010\",\n",
      "  \"Document summary\": \"This document is an application for approval of a Power Purchase Agreement for As-Available Energy with Kapaa Solar LLC by Kauai Island Utility Cooperative. The application requests the approval of the agreement by the Hawaii Public Utilities Commission and addresses the terms and conditions of the power purchase arrangements.\",\n",
      "  \"Document type\": \"PPA\",\n",
      "  \"Suggested title\": \"08-03-2010 Power Purchase Agreement Approval (main)\",\n",
      "  \"Suggested title v2\": \"08-03-2010 PPA Approval Application (main)\",\n",
      "  \"Suggested title v3\": \"08-03-2010 Energy Agreement Approval (main)\",\n",
      "  \"Suggested folder\": \"PPA\"\n",
      "}\n",
      "JSON data successfully saved to PPA.json\n"
     ]
    }
   ],
   "source": [
    "def save_json_with_pdf_name(json_str, pdf_name):\n",
    "    \"\"\"\n",
    "    Saves a JSON string to a file with the same base name as the input PDF file but with a .json extension.\n",
    "\n",
    "    Parameters:\n",
    "    - json_str (str): The JSON string to save.\n",
    "    - pdf_name (str): The filename of the PDF, used to derive the JSON filename.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Extract the base filename without the extension\n",
    "    base_name = os.path.splitext(pdf_name)[0]\n",
    "    # Construct the JSON filename\n",
    "    json_filename = f\"{base_name}.json\"\n",
    "    \n",
    "    try:\n",
    "        # Convert the JSON string to a Python dictionary\n",
    "        data = json.loads(json_str)\n",
    "        # Open the file in write mode and save the JSON\n",
    "        with open(json_filename, 'w') as file:\n",
    "            json.dump(data, file, indent=4)  # Pretty print the JSON\n",
    "        print(f\"JSON data successfully saved to {json_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving JSON to file: {e}\")\n",
    "\n",
    "set_directory_for_input_document()                      # Set directory of where to find pdf\n",
    "pdf_name = 'PPA.pdf'                                    # Define the name of the file\n",
    "extracted_text = extract_text_from_pdf_parallel(pdf_name)        # turn PDF into text\n",
    "output_extracted_text_to_file(extracted_text)           # View result of PDF to txt conversion\n",
    "query = construct_query(extracted_text)                 # Merge question with text for prompt\n",
    "truncated_query = truncate_query_to_fit_context(query)  # truncate query to fit in context length. not optimal\n",
    "\n",
    "# API call\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo-0125\",\n",
    "  #model=\"gpt-3.5-turbo-0125\" is the best available --- https://platform.openai.com/docs/models/gpt-3-5-turbo\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a solar M&A analyst and great at extracting summaries and text from M&A documentation. Under no circumstances do you halucinate, instead you say that you leave a field blank if you cannot answer\"},\n",
    "    {\"role\": \"user\", \"content\": truncated_query}\n",
    "  ]\n",
    ")\n",
    "output_json = completion.choices[0].message.content     # get message contents from api call\n",
    "\n",
    "print(output_json)                                      # print results\n",
    "save_json_with_pdf_name(output_json, pdf_name)          # save results to JSON file with same name as PDF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
