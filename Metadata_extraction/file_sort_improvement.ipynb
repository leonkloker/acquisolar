{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install and Import\n",
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "import json\n",
    "import tiktoken\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set Client and Diretory to find documents in (local)\n",
    "client = OpenAI(api_key=\"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\")\n",
    "\n",
    "def set_directory_for_input_document():\n",
    "    # Change the working directory my local one\n",
    "    target_directory = r'/Users/jd/Documents/Coding/AcquiSolar/Metadata_extraction/input/J_test'\n",
    "    os.chdir(target_directory)\n",
    "    print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Text from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "## How to process the text\n",
    "def process_extracted_text(text):\n",
    "    # Remove headers/footers\n",
    "    text = re.sub(r'(?m)^(?:\\d+|[A-Z]+)\\s*(\\r?\\n)\\1', '', text)\n",
    "    # Handle hyphenation at the end of lines\n",
    "    text = re.sub(r'(\\w+)-\\s*\\n\\s*(\\w+)', r'\\1\\2', text)\n",
    "    # Remove line breaks within a paragraph\n",
    "    text = re.sub(r'(?<!\\n)\\n(?!\\n)', ' ', text)\n",
    "    # Remove page numbers\n",
    "    text = re.sub(r'(?m)^\\s*\\d+\\s*\\n', '', text)\n",
    "    return text\n",
    "\n",
    "## How to extract the text\n",
    "def extract_text_from_page(doc, page_num):\n",
    "    page = doc.load_page(page_num)  # doc = pdf object\n",
    "    text = page.get_text()\n",
    "    # Apply the text processing here\n",
    "    processed_text = process_extracted_text(text)\n",
    "    return processed_text\n",
    "\n",
    "## How to handle different pages\n",
    "def extract_text_from_pdf_parallel(pdf_name):\n",
    "    doc = fitz.open(pdf_name)\n",
    "    full_text = \"\"\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        # Map each page to the executor\n",
    "        results = executor.map(lambda p: extract_text_from_page(doc, p), range(len(doc)))\n",
    "        for text in results:\n",
    "            full_text += text + \"\\n\"  # Concatenate the results with newlines\n",
    "    doc.close()\n",
    "    return full_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## New Version\n",
    "\n",
    "\n",
    "\n",
    "# from transformers import GPT2Tokenizer  # Placeholder for GPT-4 tokenizer\n",
    "\n",
    "# def truncate_query_to_fit_context(query, max_length=2000):\n",
    "#     \"\"\"\n",
    "#     Truncate a query using GPT-4 tokenizer to fit within a specified maximum length.\n",
    "    \n",
    "#     Parameters:\n",
    "#     - query (str): The text query to be truncated.\n",
    "#     - max_length (int): The maximum allowed length in tokens.\n",
    "    \n",
    "#     Returns:\n",
    "#     - str: Truncated query.\n",
    "#     \"\"\"\n",
    "#     # Initialize GPT-4 tokenizer (using GPT-2 as a placeholder)\n",
    "#     # For actual implementation, replace 'gpt2' with the appropriate GPT-4 identifier\n",
    "#     tokenizer = GPT2Tokenizer.from_pretrained('gpt2')  # This should be replaced with GPT-4's tokenizer\n",
    "    \n",
    "#     # Tokenize the query\n",
    "#     tokens = tokenizer.encode(query, return_tensors=\"pt\")\n",
    "    \n",
    "#     # Ensure the token array does not exceed max_length\n",
    "#     if tokens.size(1) > max_length:\n",
    "#         # Truncate the tokens to the maximum length\n",
    "#         truncated_tokens = tokens[:, :max_length]\n",
    "#         # Decode tokens back to text\n",
    "#         truncated_query = tokenizer.decode(truncated_tokens[0], clean_up_tokenization_spaces=True)\n",
    "#     else:\n",
    "#         truncated_query = query\n",
    "\n",
    "#     return truncated_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jd/opt/anaconda3/envs/genv2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "### Version 3\n",
    "from transformers import GPT2Tokenizer  \n",
    "\n",
    "\n",
    "def truncate_query_to_fit_context(query, max_length=2000):\n",
    "    \"\"\"\n",
    "    Truncate a query using GPT-2 tokenizer to fit within a specified maximum length.\n",
    "    \n",
    "    Parameters:\n",
    "    - query (str): The text query to be truncated.\n",
    "    - max_length (int): The maximum allowed length in tokens.\n",
    "    \n",
    "    Returns:\n",
    "    - str: Truncated query.\n",
    "    \"\"\"\n",
    "    # Initialize GPT-4 tokenizer (using GPT-2 as a placeholder)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2') \n",
    "    \n",
    "    # Tokenize the query and truncate if needed\n",
    "    tokens = tokenizer.encode(query, return_tensors=\"pt\")\n",
    "    num_tokens = tokens.size(1)\n",
    "    \n",
    "    if num_tokens > max_length:\n",
    "        # Truncate the tokens to the maximum length\n",
    "        truncated_tokens = tokens[:, :max_length].tolist()[0]\n",
    "        # Try to find the last complete sentence to avoid cutting in the middle of a sentence\n",
    "        end_of_sentence_indices = [idx for idx, token_id in enumerate(truncated_tokens) if tokenizer.decode([token_id]) in '.!?']\n",
    "        if end_of_sentence_indices:\n",
    "            last_sentence_index = end_of_sentence_indices[-1] + 1\n",
    "            truncated_tokens = truncated_tokens[:last_sentence_index]\n",
    "        # Decode tokens back to text\n",
    "        truncated_query = tokenizer.decode(truncated_tokens, clean_up_tokenization_spaces=True)\n",
    "    else:\n",
    "        truncated_query = query\n",
    "    \n",
    "    return truncated_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def construct_query(extracted_text):\n",
    "#     return f\"\"\"\n",
    "# Extract the following fields from the document text provided and format the response as JSON:\n",
    "# - \"Document date\" in the format '3 letter month name-DD, YYYY'.\n",
    "# - \"Document summary\" limited to a maximum of 3 sentences, tailored for a solar M&A analyst. It should state what kind of document it is, but also what its implicatoins are or what state it is in. It should assume the analyst knows about the M&A process.\n",
    "# - \"Document type\", which should be either 'PPA' or 'Interconnection document' or 'email' or 'site control'.\n",
    "# - \"Suggested title\" in the format 'MM-DD-YYYY max 5 word document title (state)' the state field is optional. It can read \"main\" if it is said to be the main document of its type, it can read (redacted) if it is redacted.\n",
    "# - \"Suggested title v2\" in same format as \"suggested title\" but with different wording\n",
    "# - \"Suggested title v3\" in same format as \"suggested title\" but with different wording\n",
    "# - \"Suggested folder\" from the selection: \"PPA\", \"interconnection\", \"uncategorized\", \"site control\"\n",
    "\n",
    "# The provided document text is:\n",
    "# {extracted_text}\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def construct_query(extracted_text):\n",
    "#     return f\"\"\"\n",
    "# Please analyze the provided document text and classify it into the most appropriate folder based on the content and context. The folders are \"PPA\", \"interconnection\", \"uncategorized\", \"site control\". Consider the following when classifying:\n",
    "\n",
    "# - Document date in 'MMM-DD, YYYY' format.\n",
    "# - A brief summary, focusing on implications and status relevant to solar M&A processes.\n",
    "# - Document type: \"PPA\", \"Interconnection document\", \"email\", \"site control\".\n",
    "# - Suggested titles in 'MM-DD-YYYY max 5 word title (state)' format. Include reasoning for state indication if applicable.\n",
    "# - Identify key phrases or keywords that strongly indicate a specific folder.\n",
    "# - Explain your reasoning for the folder suggestion, including any indicators or document characteristics that influenced your choice.\n",
    "# - If applicable, suggest multiple folders and specify the confidence level of each suggestion.\n",
    "\n",
    "# Your classification will be reviewed and used to iteratively improve our document management process.\n",
    "\n",
    "# Provided document text:\n",
    "# {extracted_text}\n",
    "# \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_metadata_query(extracted_text):\n",
    "    return f\"\"\"\n",
    "Extract the following fields from the document text provided and format the response as JSON:\n",
    "- \"Document date\" in the format '3 letter month name-DD, YYYY'.\n",
    "- \"Document summary\" limited to a maximum of 3 sentences, tailored for a solar M&A analyst.\n",
    "- \"Document type\", which should be either 'PPA', 'Interconnection document', 'email', or 'site control'.\n",
    "- \"Suggested title\", \"Suggested title v2\", and \"Suggested title v3\" in the format 'MM-DD-YYYY max 5 word document title (state)'.\n",
    "\"\"\"\n",
    "\n",
    "def construct_classification_query(metadata, document_summary):\n",
    "    # Summarize the key extracted information\n",
    "    summary = f\"Document Date: {metadata['Document date']}, Type: {metadata['Document type']}, Summary: {document_summary}\"\n",
    "    \n",
    "    # Provide explicit instructions for classification\n",
    "    instructions = \"\"\"\n",
    "    Based on the summary above and the detailed document content provided below, classify the document into one of the following folders: \"PPA\", \"interconnection\", \"uncategorized\", \"site control\". Consider the following criteria for each category:\n",
    "    - PPA: Documents related to Power Purchase Agreements.\n",
    "    - Interconnection: Documents dealing with the connection of solar power facilities to the grid.\n",
    "    - Site Control: Documents related to the ownership, lease, or control of sites for solar development.\n",
    "    - Uncategorized: Documents that do not fit into the other categories or lack enough information for a clear classification.\n",
    "\n",
    "    If uncertain, classify as 'uncategorized' but note the reason for uncertainty.\n",
    "    If PPA, interconnection or site control note reason for why it was classified this way\n",
    "    \"\"\"\n",
    "\n",
    "    # Combine the elements into the final prompt\n",
    "    prompt = f\"{summary}\\n\\n{instructions}\\n\\nThe provided document text is:\\n{extracted_text}\"\n",
    "    return prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_extracted_text_to_file(extracted_text, filename=\"Extracted_text.txt\"):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_json_with_pdf_name(json_str, pdf_name):\n",
    "#     \"\"\"\n",
    "#     Saves a JSON string to a file with the same base name as the input PDF file but with a .json extension.\n",
    "\n",
    "#     Parameters:\n",
    "#     - json_str (str): The JSON string to save.\n",
    "#     - pdf_name (str): The filename of the PDF, used to derive the JSON filename.\n",
    "\n",
    "#     Returns:\n",
    "#     - None\n",
    "#     \"\"\"\n",
    "#     # Extract the base filename without the extension\n",
    "#     base_name = os.path.splitext(pdf_name)[0]\n",
    "#     # Construct the JSON filename\n",
    "#     json_filename = f\"{base_name}.json\"\n",
    "    \n",
    "#     try:\n",
    "#         if json_str.strip():  # Check if json_str is not empty\n",
    "#             # Convert the JSON string to a Python dictionary\n",
    "#             data = json.loads(json_str)\n",
    "#             # Open the file in write mode and save the JSON\n",
    "#             with open(json_filename, 'w') as file:\n",
    "#                 json.dump(data, file, indent=4)  # Pretty print the JSON\n",
    "#             print(f\"JSON data successfully saved to {json_filename}\")\n",
    "#         else:\n",
    "#             print(\"Received empty JSON string, skipping file saving.\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error saving JSON to file: {e}\")\n",
    "\n",
    "# set_directory_for_input_document()                      # Set directory of where to find pdf\n",
    "# pdf_name = 'PPA.pdf'                                    # Define the name of the file\n",
    "# extracted_text = extract_text_from_pdf_parallel(pdf_name)        # turn PDF into text\n",
    "# output_extracted_text_to_file(extracted_text)           # View result of PDF to txt conversion\n",
    "# query = construct_query(extracted_text)                 # Merge question with text for prompt\n",
    "# truncated_query = truncate_query_to_fit_context(query)  # truncate query to fit in context length. not optimal\n",
    "\n",
    "# # API call\n",
    "# completion = client.chat.completions.create(\n",
    "#   model=\"gpt-3.5-turbo-0125\",\n",
    "#   #model=\"gpt-3.5-turbo-0125\" is the best available --- https://platform.openai.com/docs/models/gpt-3-5-turbo\n",
    "#   messages=[\n",
    "#     {\"role\": \"system\", \"content\": \"You are a solar M&A analyst and great at extracting summaries and text from M&A documentation. Under no circumstances do you halucinate, instead you say that you leave a field blank if you cannot answer\"},\n",
    "#     {\"role\": \"user\", \"content\": truncated_query}\n",
    "#   ]\n",
    "# )\n",
    "# output_json = completion.choices[0].message.content     # get message contents from api call\n",
    "\n",
    "# print(output_json)                                      # print results\n",
    "# save_json_with_pdf_name(output_json, pdf_name)          # save results to JSON file with same name as PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_completion(completion):\n",
    "    \"\"\"\n",
    "    Processes the completion response from the OpenAI API to extract relevant information.\n",
    "\n",
    "    The function is designed to parse the API response and extract the necessary data for further processing or decision-making. It should handle variations in the response format gracefully and ensure that the extracted information is accurate and usable.\n",
    "\n",
    "    Parameters:\n",
    "    - completion: The response object from the OpenAI API call. This object contains the model's output and possibly other metadata related to the request.\n",
    "\n",
    "    Returns:\n",
    "    - A structured representation of the extracted information, which could be a dictionary, a list, or a simple string, depending on the expected format of the response and the needs of the application.\n",
    "\n",
    "    This function must be implemented with an understanding of the response structure returned by the specific OpenAI model being used. For instance, if the model returns JSON-formatted strings containing the desired data, the function should parse this JSON and extract the relevant fields.\n",
    "\n",
    "    It is also responsible for error handling, ensuring that any issues with the API response, such as unexpected formats or missing data, are caught and managed appropriately.\n",
    "\n",
    "    Example Usage:\n",
    "    Suppose the API response includes a text field with JSON content. In that case, the function might look like this:\n",
    "\n",
    "        try:\n",
    "            response_data = json.loads(completion.choices[0].text.strip())\n",
    "            extracted_data = {\n",
    "                'metadata': response_data['metadata'],\n",
    "                'classification': response_data['classification']\n",
    "            }\n",
    "            return extracted_data\n",
    "        except (KeyError, json.JSONDecodeError) as e:\n",
    "            print(f\"Error processing completion response: {e}\")\n",
    "            return None\n",
    "\n",
    "    Note: The above code is an example. The actual implementation will depend on the format of the API response and the specific data you need to extract.\n",
    "    \"\"\"\n",
    "    pass  # Implementation goes here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LAST PROCESS\n",
    "\n",
    "def extract_metadata_and_classify(extracted_text):\n",
    "    # First, extract metadata\n",
    "    metadata_query = construct_metadata_query(extracted_text)\n",
    "    metadata_completion = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-0125\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"Extract metadata\"}, {\"role\": \"user\", \"content\": metadata_query}]\n",
    "    )\n",
    "    metadata = process_completion(metadata_completion)  # You'll need to implement this based on your data structure\n",
    "\n",
    "    # Then, classify the document\n",
    "    classification_query = construct_classification_query(metadata, extracted_text)\n",
    "    classification_completion = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-0125\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"Classify document\"}, {\"role\": \"user\", \"content\": classification_query}]\n",
    "    )\n",
    "    classification = process_completion(classification_completion)  # Implement based on your needs\n",
    "\n",
    "    return metadata, classification\n",
    "\n",
    "def save_json_with_pdf_name(json_str, pdf_name):\n",
    "    \"\"\"\n",
    "    Saves a JSON string to a file with the same base name as the input PDF file but with a .json extension.\n",
    "\n",
    "    Parameters:\n",
    "    - json_str (str): The JSON string to save.\n",
    "    - pdf_name (str): The filename of the PDF, used to derive the JSON filename.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Extract the base filename without the extension\n",
    "    base_name = os.path.splitext(pdf_name)[0]\n",
    "    # Construct the JSON filename\n",
    "    json_filename = f\"{base_name}.json\"\n",
    "\n",
    "    if not json_str.strip():\n",
    "        print(\"Received empty or invalid JSON string, skipping file saving.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Directly save the JSON string to a file\n",
    "        with open(json_filename, 'w') as file:\n",
    "            file.write(json_str)  # Assuming json_str is already a valid JSON string\n",
    "        print(f\"JSON data successfully saved to {json_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving JSON to file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/jd/Documents/Coding/AcquiSolar/Metadata_extraction/input/J_test\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m output_extracted_text_to_file(extracted_text)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Extract metadata and classify the document\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m metadata, classification \u001b[38;5;241m=\u001b[39m \u001b[43mextract_metadata_and_classify\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextracted_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Combine metadata and classification into a single JSON object for saving\u001b[39;00m\n\u001b[1;32m     19\u001b[0m combined_results \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps({\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassification\u001b[39m\u001b[38;5;124m\"\u001b[39m: classification\n\u001b[1;32m     22\u001b[0m }, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 13\u001b[0m, in \u001b[0;36mextract_metadata_and_classify\u001b[0;34m(extracted_text)\u001b[0m\n\u001b[1;32m     10\u001b[0m metadata \u001b[38;5;241m=\u001b[39m process_completion(metadata_completion)  \u001b[38;5;66;03m# You'll need to implement this based on your data structure\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Then, classify the document\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m classification_query \u001b[38;5;241m=\u001b[39m \u001b[43mconstruct_classification_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextracted_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m classification_completion \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     15\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo-0125\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassify document\u001b[39m\u001b[38;5;124m\"\u001b[39m}, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: classification_query}]\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     18\u001b[0m classification \u001b[38;5;241m=\u001b[39m process_completion(classification_completion)  \u001b[38;5;66;03m# Implement based on your needs\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m, in \u001b[0;36mconstruct_classification_query\u001b[0;34m(metadata, document_summary)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstruct_classification_query\u001b[39m(metadata, document_summary):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Summarize the key extracted information\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocument Date: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDocument date\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDocument type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Summary: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdocument_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Provide explicit instructions for classification\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     instructions \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124m    Based on the summary above and the detailed document content provided below, classify the document into one of the following folders: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPPA\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minterconnection\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muncategorized\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msite control\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. Consider the following criteria for each category:\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124m    - PPA: Documents related to Power Purchase Agreements.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124m    If PPA, interconnection or site control note reason for why it was classified this way\u001b[39m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "### LAST PART\n",
    "\n",
    "# Set the directory where to find the PDF\n",
    "set_directory_for_input_document()\n",
    "\n",
    "# Define the name of the file\n",
    "pdf_name = 'PPA.pdf'\n",
    "\n",
    "# Extract text from the PDF\n",
    "extracted_text = extract_text_from_pdf_parallel(pdf_name)\n",
    "\n",
    "# Optionally, save extracted text to a file\n",
    "output_extracted_text_to_file(extracted_text)\n",
    "\n",
    "# Extract metadata and classify the document\n",
    "metadata, classification = extract_metadata_and_classify(extracted_text)\n",
    "\n",
    "# Combine metadata and classification into a single JSON object for saving\n",
    "combined_results = json.dumps({\n",
    "    \"metadata\": metadata,\n",
    "    \"classification\": classification\n",
    "}, indent=4)\n",
    "\n",
    "# Save the combined results to a JSON file with the same name as the PDF\n",
    "save_json_with_pdf_name(combined_results, pdf_name)\n",
    "       # save results to JSON file with same name as PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def extract_metadata_and_classify(extracted_text):\n",
    "#     # First, extract metadata\n",
    "#     metadata_query = construct_metadata_query(extracted_text)\n",
    "#     metadata_completion = client.chat.completions.create(\n",
    "#         model=\"gpt-3.5-turbo-0125\",\n",
    "#         messages=[{\"role\": \"system\", \"content\": \"Extract metadata\"}, {\"role\": \"user\", \"content\": metadata_query}]\n",
    "#     )\n",
    "#     metadata = process_completion(metadata_completion)  # You'll need to implement this based on your data structure\n",
    "\n",
    "#     # Then, classify the document\n",
    "#     classification_query = construct_classification_query(metadata, extracted_text)\n",
    "#     classification_completion = client.chat.completions.create(\n",
    "#         model=\"gpt-3.5-turbo-0125\",\n",
    "#         messages=[{\"role\": \"system\", \"content\": \"Classify document\"}, {\"role\": \"user\", \"content\": classification_query}]\n",
    "#     )\n",
    "#     classification = process_completion(classification_completion)  # Implement based on your needs\n",
    "\n",
    "#     return metadata, classification\n",
    "\n",
    "# def save_results(metadata, classification, pdf_name):\n",
    "#     # Combine metadata and classification into one JSON object\n",
    "#     combined_results = {\n",
    "#         \"metadata\": metadata,\n",
    "#         \"classification\": classification\n",
    "#     }\n",
    "#     json_str = json.dumps(combined_results, indent=4)\n",
    "#     save_json_with_pdf_name(json_str, pdf_name)\n",
    "\n",
    "# # Workflow\n",
    "# set_directory_for_input_document()  # Set directory of where to find pdf\n",
    "# pdf_name = 'PPA.pdf'  # Define the name of the file\n",
    "# extracted_text = extract_text_from_pdf_parallel(pdf_name)  # Extract text from PDF\n",
    "# output_extracted_text_to_file(extracted_text)  # Optionally, save extracted text to file\n",
    "\n",
    "# # Extract metadata and classify\n",
    "# metadata, classification = extract_metadata_and_classify(extracted_text)\n",
    "# save_results(metadata, classification, pdf_name)  # Save results to JSON file with the same name as PDF\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
