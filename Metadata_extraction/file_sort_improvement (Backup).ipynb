{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install and Import\n",
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "import json\n",
    "import tiktoken\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 02/11\n",
    "\n",
    "1) Extract text from PDF\n",
    "2) Remove elements that are unnecessary (tables or other non-essential elements)\n",
    "3) Truncate (to limit size)\n",
    "4) Create prompt to classify\n",
    "5) Use JSON mode (classify this set of pages in these categories)\n",
    "6) Create few shot prompt in JSON strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set Client and Diretory to find documents in (local)\n",
    "client = OpenAI(api_key=\"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\")\n",
    "\n",
    "def set_directory_for_input_document():\n",
    "    # Change the working directory my local one\n",
    "    target_directory = r'/Users/jd/Documents/Coding/AcquiSolar/Metadata_extraction/input/J_test'\n",
    "    os.chdir(target_directory)\n",
    "    print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Text from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## New Version: adds a function to read pages parallel to speed up the process and make it less expensive\n",
    "# ## and should it make easier to read different kinds of documents\n",
    "\n",
    "# def extract_text_from_page(doc, page_num):\n",
    "#     page = doc.load_page(page_num) ## doc = pdf object\n",
    "#     text = page.get_text()\n",
    "#     # Apply any necessary text processing here\n",
    "#     return text\n",
    "\n",
    "# def extract_text_from_pdf_parallel(pdf_name):\n",
    "#     doc = fitz.open(pdf_name)\n",
    "#     full_text = \"\"\n",
    "#     with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "#         # Map each page to the executor\n",
    "#         results = executor.map(lambda p: extract_text_from_page(doc, p), range(len(doc)))\n",
    "#         for text in results:\n",
    "#             full_text += text + \"\\n\"  # Concatenate the results with newlines\n",
    "#     doc.close()\n",
    "#     return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def process_extracted_text(text):\n",
    "    # Remove headers/footers\n",
    "    text = re.sub(r'(?m)^(?:\\d+|[A-Z]+)\\s*(\\r?\\n)\\1', '', text)\n",
    "    # Handle hyphenation at the end of lines\n",
    "    text = re.sub(r'(\\w+)-\\s*\\n\\s*(\\w+)', r'\\1\\2', text)\n",
    "    # Remove line breaks within a paragraph\n",
    "    text = re.sub(r'(?<!\\n)\\n(?!\\n)', ' ', text)\n",
    "    # Remove page numbers\n",
    "    text = re.sub(r'(?m)^\\s*\\d+\\s*\\n', '', text)\n",
    "    return text\n",
    "\n",
    "def extract_text_from_page(doc, page_num):\n",
    "    page = doc.load_page(page_num)  # doc = pdf object\n",
    "    text = page.get_text()\n",
    "    # Apply the text processing here\n",
    "    processed_text = process_extracted_text(text)\n",
    "    return processed_text\n",
    "\n",
    "def extract_text_from_pdf_parallel(pdf_name):\n",
    "    doc = fitz.open(pdf_name)\n",
    "    full_text = \"\"\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        # Map each page to the executor\n",
    "        results = executor.map(lambda p: extract_text_from_page(doc, p), range(len(doc)))\n",
    "        for text in results:\n",
    "            full_text += text + \"\\n\"  # Concatenate the results with newlines\n",
    "    doc.close()\n",
    "    return full_text\n",
    "\n",
    "# Example of using the function:\n",
    "# pdf_text = extract_text_from_pdf_parallel('path_to_your_pdf.pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Version 3\n",
    "from transformers import GPT2Tokenizer  \n",
    "\n",
    "\n",
    "def truncate_query_to_fit_context(query, max_length=2000):\n",
    "    \"\"\"\n",
    "    Truncate a query using GPT-2 tokenizer to fit within a specified maximum length.\n",
    "    \n",
    "    Parameters:\n",
    "    - query (str): The text query to be truncated.\n",
    "    - max_length (int): The maximum allowed length in tokens.\n",
    "    \n",
    "    Returns:\n",
    "    - str: Truncated query.\n",
    "    \"\"\"\n",
    "    # Initialize GPT-4 tokenizer (using GPT-2 as a placeholder)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2') \n",
    "    \n",
    "    # Tokenize the query and truncate if needed\n",
    "    tokens = tokenizer.encode(query, return_tensors=\"pt\")\n",
    "    num_tokens = tokens.size(1)\n",
    "    \n",
    "    if num_tokens > max_length:\n",
    "        # Truncate the tokens to the maximum length\n",
    "        truncated_tokens = tokens[:, :max_length].tolist()[0]\n",
    "        # Try to find the last complete sentence to avoid cutting in the middle of a sentence\n",
    "        end_of_sentence_indices = [idx for idx, token_id in enumerate(truncated_tokens) if tokenizer.decode([token_id]) in '.!?']\n",
    "        if end_of_sentence_indices:\n",
    "            last_sentence_index = end_of_sentence_indices[-1] + 1\n",
    "            truncated_tokens = truncated_tokens[:last_sentence_index]\n",
    "        # Decode tokens back to text\n",
    "        truncated_query = tokenizer.decode(truncated_tokens, clean_up_tokenization_spaces=True)\n",
    "    else:\n",
    "        truncated_query = query\n",
    "    \n",
    "    return truncated_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_query(extracted_text):\n",
    "    return f\"\"\"\n",
    "Extract the following fields from the document text provided and format the response as JSON:\n",
    "- \"Document date\" in the format '3 letter month name-DD, YYYY'.\n",
    "- \"Document summary\" limited to a maximum of 3 sentences, tailored for a solar M&A analyst. It should state what kind of document it is, but also what its implicatoins are or what state it is in. It should assume the analyst knows about the M&A process.\n",
    "- \"Document type\", which should be either 'PPA' or 'Interconnection document' or 'email' or 'site control'.\n",
    "- \"Suggested title\" in the format 'MM-DD-YYYY max 5 word document title (state)' the state field is optional. It can read \"main\" if it is said to be the main document of its type, it can read (redacted) if it is redacted.\n",
    "- \"Suggested title v2\" in same format as \"suggested title\" but with different wording\n",
    "- \"Suggested title v3\" in same format as \"suggested title\" but with different wording\n",
    "- \"Suggested folder\" from the selection: \"PPA\", \"interconnection\", \"uncategorized\", \"site control\"\n",
    "- \"Reasoning\": Give a one sentence reasoning for your suggestion of the folder selection\n",
    "\n",
    "The provided document text is:\n",
    "{extracted_text}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_extracted_text_to_file(extracted_text, filename=\"Extracted_text.txt\"):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/jd/Documents/Coding/AcquiSolar/Metadata_extraction/input/J_test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21409 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"Document date\": \"AUG-03, 2010\",\n",
      "    \"Document summary\": \"This document is an application for the approval of a Power Purchase Agreement (PPA) for as-available energy with Kapaa Solar LLC by Kauai Island Utility Cooperative. The application is in the stage of requesting approval from the Hawaii Public Utilities Commission for the PPA and related energy rate adjustments.\",\n",
      "    \"Document type\": \"PPA\",\n",
      "    \"Suggested title\": \"08-03-2010 PPA Document (main)\",\n",
      "    \"Suggested title v2\": \"08-03-2010 Power Purchase Agreement Request\",\n",
      "    \"Suggested title v3\": \"08-03-2010 Energy Rate Adjustment Application\",\n",
      "    \"Suggested folder\": \"PPA\",\n",
      "    \"Reasoning\": \"The document primarily revolves around the approval and terms of a Power Purchase Agreement, hence falling under the 'PPA' category.\"\n",
      "}\n",
      "```\n",
      "Error saving JSON to file: Expecting value: line 1 column 1 (char 0)\n"
     ]
    }
   ],
   "source": [
    "def save_json_with_pdf_name(json_str, pdf_name):\n",
    "    \"\"\"\n",
    "    Saves a JSON string to a file with the same base name as the input PDF file but with a .json extension.\n",
    "\n",
    "    Parameters:\n",
    "    - json_str (str): The JSON string to save.\n",
    "    - pdf_name (str): The filename of the PDF, used to derive the JSON filename.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Extract the base filename without the extension\n",
    "    base_name = os.path.splitext(pdf_name)[0]\n",
    "    # Construct the JSON filename\n",
    "    json_filename = f\"{base_name}.json\"\n",
    "    \n",
    "    try:\n",
    "        # Convert the JSON string to a Python dictionary\n",
    "        data = json.loads(json_str)\n",
    "        # Open the file in write mode and save the JSON\n",
    "        with open(json_filename, 'w') as file:\n",
    "            json.dump(data, file, indent=4)  # Pretty print the JSON\n",
    "        print(f\"JSON data successfully saved to {json_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving JSON to file: {e}\")\n",
    "\n",
    "set_directory_for_input_document()                      # Set directory of where to find pdf\n",
    "pdf_name = 'PPA.pdf'                                    # Define the name of the file\n",
    "extracted_text = extract_text_from_pdf_parallel(pdf_name)        # turn PDF into text\n",
    "output_extracted_text_to_file(extracted_text)           # View result of PDF to txt conversion\n",
    "query = construct_query(extracted_text)                 # Merge question with text for prompt\n",
    "truncated_query = truncate_query_to_fit_context(query)  # truncate query to fit in context length. not optimal\n",
    "\n",
    "# API call\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo-0125\",\n",
    "  #model=\"gpt-3.5-turbo-0125\" is the best available --- https://platform.openai.com/docs/models/gpt-3-5-turbo\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a solar M&A analyst and great at extracting summaries and text from M&A documentation. Under no circumstances do you halucinate, instead you say that you leave a field blank if you cannot answer\"},\n",
    "    {\"role\": \"user\", \"content\": truncated_query}\n",
    "  ]\n",
    ")\n",
    "output_json = completion.choices[0].message.content     # get message contents from api call\n",
    "\n",
    "print(output_json)                                      # print results\n",
    "save_json_with_pdf_name(output_json, pdf_name)          # save results to JSON file with same name as PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_document(text: str, client: OpenAI) -> str:\n",
    "    \"\"\"\n",
    "    Categorize a document based on its content using GPT-3.5.\n",
    "\n",
    "    Args:\n",
    "    text (str): The processed text of the document.\n",
    "    client (OpenAI): The OpenAI API client.\n",
    "\n",
    "    Returns:\n",
    "    str: The category of the document.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Please categorize the following document into one of these categories:\n",
    "    PPA, interconnection, uncategorized, site control.\n",
    "\n",
    "    Document:\n",
    "    {text}\n",
    "\n",
    "    Which category does it belong to?\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.create_completion(\n",
    "        model=\"gpt-3.5-turbo-0125\",\n",
    "        prompt=prompt,\n",
    "        temperature=0.5,  # Adjust as needed for creativity vs. specificity\n",
    "        max_tokens=60  # Adjust based on expected length of response\n",
    "    )\n",
    "\n",
    "    # Assuming the response directly provides the category\n",
    "    category = response.choices[0].text.strip()\n",
    "\n",
    "    # Additional logic might be needed here to validate or parse the response\n",
    "\n",
    "    return category\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
