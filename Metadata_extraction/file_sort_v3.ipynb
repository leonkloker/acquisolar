{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install and Import\n",
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "import json\n",
    "import tiktoken\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set Client and Diretory to find documents in (local)\n",
    "client = OpenAI(api_key=\"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\")\n",
    "\n",
    "def set_directory_for_input_document():\n",
    "    # Change the working directory my local one\n",
    "    target_directory = r'/Users/jd/Documents/Coding/AcquiSolar/Metadata_extraction/input/J_test'\n",
    "    os.chdir(target_directory)\n",
    "    print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import List\n",
    "\n",
    "def process_extracted_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Process the extracted text from a PDF page by removing headers, footers,\n",
    "    handling hyphenation, and removing unwanted line breaks and page numbers.\n",
    "\n",
    "    Args:\n",
    "    text (str): The extracted text from a PDF page.\n",
    "\n",
    "    Returns:\n",
    "    str: The processed text.\n",
    "    \"\"\"\n",
    "    # Remove headers/footers\n",
    "    text = re.sub(r'(?m)^(?:\\d+|[A-Z]+)\\s*(\\r?\\n)\\1', '', text)\n",
    "    # Handle hyphenation at the end of lines\n",
    "    text = re.sub(r'(\\w+)-\\s*\\n\\s*(\\w+)', r'\\1\\2', text)\n",
    "    # Remove line breaks within a paragraph\n",
    "    text = re.sub(r'(?<!\\n)\\n(?!\\n)', ' ', text)\n",
    "    # Remove page numbers\n",
    "    text = re.sub(r'(?m)^\\s*\\d+\\s*\\n', '', text)\n",
    "    return text\n",
    "\n",
    "def extract_text_from_page(doc: fitz.Document, page_num: int) -> str:\n",
    "    \"\"\"\n",
    "    Extract and process text from a specific page of a PDF document.\n",
    "\n",
    "    Args:\n",
    "    doc (fitz.Document): The PDF document object.\n",
    "    page_num (int): The page number to extract text from.\n",
    "\n",
    "    Returns:\n",
    "    str: The processed text from the specified page. Returns an empty string on error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        page = doc.load_page(page_num)  # Load the specified page\n",
    "        text = page.get_text()\n",
    "        processed_text = process_extracted_text(text)  # Apply text processing\n",
    "        return processed_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing page {page_num}: {e}\")\n",
    "        return \"\"  # Return empty string on error\n",
    "\n",
    "def extract_text_from_pdf_parallel(pdf_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract and process text from all pages of a PDF document in parallel.\n",
    "\n",
    "    Args:\n",
    "    pdf_name (str): The file path of the PDF document.\n",
    "\n",
    "    Returns:\n",
    "    str: The concatenated processed text from all pages of the document.\n",
    "         Returns an empty string if the document cannot be opened or processed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc = fitz.open(pdf_name)  # Attempt to open the PDF document\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening PDF {pdf_name}: {e}\")\n",
    "        return \"\"  # Return empty string if the document cannot be opened\n",
    "\n",
    "    num_pages = len(doc)\n",
    "    max_workers = min(4, os.cpu_count() or 1, num_pages)  # Dynamically set max_workers\n",
    "    \n",
    "    full_text: List[str] = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Map each page to the executor\n",
    "        results = executor.map(lambda p: extract_text_from_page(doc, p), range(num_pages))\n",
    "        full_text = [text for text in results]  # Collect processed text\n",
    "    \n",
    "    doc.close()\n",
    "    return ''.join(full_text)  # Concatenate processed texts into a single string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Up to here it was sound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I'll import the old parts to see how it works\n",
    "### Old version works. Gives reasoning. Let's improve it from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jd/opt/anaconda3/envs/genv2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "### Version 3\n",
    "from transformers import GPT2Tokenizer  \n",
    "\n",
    "\n",
    "def truncate_query_to_fit_context(query, max_length=2000):\n",
    "    \"\"\"\n",
    "    Truncate a query using GPT-2 tokenizer to fit within a specified maximum length.\n",
    "    \n",
    "    Parameters:\n",
    "    - query (str): The text query to be truncated.\n",
    "    - max_length (int): The maximum allowed length in tokens.\n",
    "    \n",
    "    Returns:\n",
    "    - str: Truncated query.\n",
    "    \"\"\"\n",
    "    # Initialize GPT-4 tokenizer (using GPT-2 as a placeholder)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2') \n",
    "    \n",
    "    # Tokenize the query and truncate if needed\n",
    "    tokens = tokenizer.encode(query, return_tensors=\"pt\")\n",
    "    num_tokens = tokens.size(1)\n",
    "    \n",
    "    if num_tokens > max_length:\n",
    "        # Truncate the tokens to the maximum length\n",
    "        truncated_tokens = tokens[:, :max_length].tolist()[0]\n",
    "        # Try to find the last complete sentence to avoid cutting in the middle of a sentence\n",
    "        end_of_sentence_indices = [idx for idx, token_id in enumerate(truncated_tokens) if tokenizer.decode([token_id]) in '.!?']\n",
    "        if end_of_sentence_indices:\n",
    "            last_sentence_index = end_of_sentence_indices[-1] + 1\n",
    "            truncated_tokens = truncated_tokens[:last_sentence_index]\n",
    "        # Decode tokens back to text\n",
    "        truncated_query = tokenizer.decode(truncated_tokens, clean_up_tokenization_spaces=True)\n",
    "    else:\n",
    "        truncated_query = query\n",
    "    \n",
    "    return truncated_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_query(extracted_text):\n",
    "    return f\"\"\"\n",
    "Extract the following fields from the document text provided and format the response as JSON:\n",
    "- \"Document date\" in the format '3 letter month name-DD, YYYY'.\n",
    "- \"Document summary\" limited to a maximum of 3 sentences, tailored for a solar M&A analyst. It should state what kind of document it is, but also what its implicatoins are or what state it is in. It should assume the analyst knows about the M&A process.\n",
    "- \"Document type\", which should be either 'PPA' or 'Interconnection document' or 'email' or 'site control'.\n",
    "- \"Suggested title\" in the format 'MM-DD-YYYY max 5 word document title (state)' the state field is optional. It can read \"main\" if it is said to be the main document of its type, it can read (redacted) if it is redacted.\n",
    "- \"Suggested title v2\" in same format as \"suggested title\" but with different wording\n",
    "- \"Suggested title v3\" in same format as \"suggested title\" but with different wording\n",
    "- \"Suggested folder v1\" from the selection: \"PPA\", \"interconnection\", \"uncategorized\", \"site control\"\n",
    "- \"Suggested folder v2\" from the selection: \"PPA\", \"interconnection\", \"uncategorized\", \"site control\"\n",
    "- \"Certaintiy\": state certainty from 0% to 100% for your selection for V1\n",
    "- \"Reasoning\": Give a one sentence reasoning for your suggestion of the folder selection\n",
    "\n",
    "The provided document text is:\n",
    "{extracted_text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_extracted_text_to_file(extracted_text, filename=\"Extracted_text.txt\"):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/jd/Documents/Coding/AcquiSolar/Metadata_extraction/input/J_test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1110 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Document date\": \"MAR-28, 2023\",\n",
      "    \"Document summary\": \"This document is a letter notifying the Public Utilities Commission of Hawaii that Kupono Solar, LLC is ready for decision making in Docket No. 2022-0007. The Consumer Advocate does not object to the proposed transmission line extensions, signaling progress in the proceeding.\",\n",
      "    \"Document type\": \"email\",\n",
      "    \"Suggested title\": \"03-28-2023 Notification of Readiness (redacted)\",\n",
      "    \"Suggested title v2\": \"03-28-2023 Decision Making Update\",\n",
      "    \"Suggested title v3\": \"03-28-2023 Progress Notice\",\n",
      "    \"Suggested folder v1\": \"uncategorized\",\n",
      "    \"Suggested folder v2\": \"PPA\",\n",
      "    \"Certainty\": \"80%\",\n",
      "    \"Reasoning\": \"The email contains updates on decision making and progress in the M&A proceeding, hence falls under 'uncategorized' and 'PPA'.\"\n",
      "}\n",
      "JSON data successfully saved to 00006158.json\n"
     ]
    }
   ],
   "source": [
    "def save_json_with_pdf_name(json_str, pdf_name):\n",
    "    \"\"\"\n",
    "    Saves a JSON string to a file with the same base name as the input PDF file but with a .json extension.\n",
    "\n",
    "    Parameters:\n",
    "    - json_str (str): The JSON string to save.\n",
    "    - pdf_name (str): The filename of the PDF, used to derive the JSON filename.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Extract the base filename without the extension\n",
    "    base_name = os.path.splitext(pdf_name)[0]\n",
    "    # Construct the JSON filename\n",
    "    json_filename = f\"{base_name}.json\"\n",
    "    \n",
    "    try:\n",
    "        # Convert the JSON string to a Python dictionary\n",
    "        data = json.loads(json_str)\n",
    "        # Open the file in write mode and save the JSON\n",
    "        with open(json_filename, 'w') as file:\n",
    "            json.dump(data, file, indent=4)  # Pretty print the JSON\n",
    "        print(f\"JSON data successfully saved to {json_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving JSON to file: {e}\")\n",
    "\n",
    "set_directory_for_input_document()                      # Set directory of where to find pdf\n",
    "# choose from \n",
    "# 00005CC7.pdf ; uncategorized\n",
    "# 0000602B.pdf ; unknown (PPA)\n",
    "# 00005354.pdf ; uncategorized\n",
    "# 00006158.pdf ; uncategorized\n",
    "# 00006727.pdf ; uncategorized\n",
    "# PPA.pdf ; correct (PPA)\n",
    "pdf_name = '00006158.pdf'                                    # Define the name of the file\n",
    "extracted_text = extract_text_from_pdf_parallel(pdf_name)        # turn PDF into text\n",
    "output_extracted_text_to_file(extracted_text)           # View result of PDF to txt conversion\n",
    "query = construct_query(extracted_text)                 # Merge question with text for prompt\n",
    "truncated_query = truncate_query_to_fit_context(query)  # truncate query to fit in context length. not optimal\n",
    "\n",
    "# API call\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo-0125\",\n",
    "  #model=\"gpt-3.5-turbo-0125\" is the best available --- https://platform.openai.com/docs/models/gpt-3-5-turbo\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a solar M&A analyst and great at extracting summaries and text from M&A documentation. Under no circumstances do you halucinate, instead you say that you leave a field blank if you cannot answer\"},\n",
    "    {\"role\": \"user\", \"content\": truncated_query}\n",
    "  ]\n",
    ")\n",
    "output_json = completion.choices[0].message.content     # get message contents from api call\n",
    "\n",
    "print(output_json)                                      # print results\n",
    "save_json_with_pdf_name(output_json, pdf_name)          # save results to JSON file with same name as PDF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
